{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import neighbors\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.regularizers import l2\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dropout\nimport tensorflow as tf\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nimport dateutil\nfrom category_encoders import TargetEncoder\nfrom sklearn import preprocessing\nimport datetime\nfrom PIL import ImageColor\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training = pd.read_csv(\"/kaggle/input/ift6758-a20/train.csv\")\ntesting  = pd.read_csv(\"/kaggle/input/ift6758-a20/test.csv\")\n\n\ntraining['Location Public Visibility'] = training['Location Public Visibility'].str.lower()\n# User Name column is not relevent\ntraining.drop('User Name', 1,inplace=True)\ntraining.drop('User Time Zone', 1,inplace=True)\ntraining.drop('Profile Image', 1,inplace=True)\ntraining.drop('Location', 1,inplace=True)\n\n# mean fill nan for categorical features\ntraining['Profile Cover Image Status'].fillna('Set', inplace=True)\ntraining['UTC Offset'].fillna(training['UTC Offset'].mean(), inplace=True)\ntraining['Profile Theme Color'].fillna(0,inplace = True)\ntraining['Profile Text Color'].fillna(0,inplace = True)\ntraining['Profile Page Color'].fillna(0,inplace = True)\n\n# One hot encodeing some of the categorical variables\ntraining = pd.get_dummies(training, columns = ['Profile Verification Status'])\ntraining = pd.get_dummies(training, columns = ['Location Public Visibility'])\n# training = pd.get_dummies(training, columns = ['Profile Category'])\n\n# Binary encoding some Cathegorical variables as binary\ntraining['Personal URL'] = training['Personal URL'].notnull().astype('int')\ntraining['Profile Cover Image Status'] = np.where(training['Profile Cover Image Status']=='Set', 1, 0)\ntraining['Is Profile View Size Customized?'] = training['Is Profile View Size Customized?'].astype(int)\n# training['User Language'] = np.where(training['User Language']=='en', 1, 0)\n\n# Mean imputation for 'Avg Daily Profile Clicks'And'Avg Daily Profile Visit Duration in seconds'\nimp_median = SimpleImputer( strategy='mean') #for median imputation replace 'mean' with 'median'\nimp_median.fit(training[['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds']])\nimputed_train_df = imp_median.transform(training[['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds']])\nimputed_train_df = pd.DataFrame(imputed_train_df,columns=['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds'])\ntraining[['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds']] = imputed_train_df[['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds']]\n\n# target encoding UTC, Profile Category, Language\nt_encod_prof_cat=TargetEncoder(cols=['Profile Category']).fit(training['Profile Category'], training['Num of Profile Likes'])\nt_encod_prof_utc=TargetEncoder(cols=['UTC Offset']).fit(training['UTC Offset'], training['Num of Profile Likes'])\nt_encod_prof_lang=TargetEncoder(cols=['User Language']).fit(training['User Language'], training['Num of Profile Likes'])\ntraining['Profile Category'] = t_encod_prof_cat.transform(training['Profile Category'])\ntraining['UTC Offset'] = t_encod_prof_utc.transform(training['UTC Offset'])\ntraining['User Language'] = t_encod_prof_lang.transform(training['User Language'])\n\n\n\n# Log transform of the numerical features\ntraining['Num of People Following'] = np.log(training['Num of People Following'])\ntraining['Num of Status Updates'] = np.log(training['Num of Status Updates'])\ntraining['Avg Daily Profile Clicks'] = np.log(training['Avg Daily Profile Clicks'])\ntraining['Num of Followers'] = np.log(training['Num of Followers'])\ntraining['Avg Daily Profile Visit Duration in seconds'] = np.log(training['Avg Daily Profile Visit Duration in seconds'])\ntraining['Num of Direct Messages'] = np.log(training['Num of Direct Messages'])\n\n# robust scale the nemerical features\ntraining = training.replace([np.inf, -np.inf], 0)\n# scaler = preprocessing.RobustScaler()\n# robust_scaled_df = scaler.fit_transform(training[['Num of People Following','Num of Status Updates','Avg Daily Profile Clicks','Num of Followers','Avg Daily Profile Visit Duration in seconds','Num of Direct Messages']])\n# training[['Num of People Following','Num of Status Updates','Avg Daily Profile Clicks','Num of Followers','Avg Daily Profile Visit Duration in seconds','Num of Direct Messages']] = pd.DataFrame(robust_scaled_df, columns=['Num of People Following','Num of Status Updates','Avg Daily Profile Clicks','Num of Followers','Avg Daily Profile Visit Duration in seconds','Num of Direct Messages'])\n\n\n# UTF off-set frequency encoding\n# enc_time_zone = (training.groupby('UTC Offset').size()) / len(training)\n# training['UTC Offset'] = training['UTC Offset'].apply(lambda x : enc_time_zone[x])\n\n\n# colors encoding, convert to int\n#     training['Profile Theme Color'].fillna(0,inplace = True)\n#     training['Profile Theme Color']=training['Profile Theme Color'].apply(lambda x: int(str(x), 16))\n\n#     training['Profile Page Color'].fillna(0,inplace = True)\n#     training['Profile Page Color']=training['Profile Page Color'].apply(lambda x: int(str(x), 16))\n\n#     training['Profile Text Color'].fillna(0,inplace = True)\n#     training['Profile Text Color']=training['Profile Text Color'].apply(lambda x: int(str(x), 16))\n\n# enc_color_theme = (training.groupby('Profile Theme Color').size()) / len(training)\n# training['Profile Theme Color'] = training['Profile Theme Color'].apply(lambda x : enc_color_theme[x])\n\n# enc_color_text = (training.groupby('Profile Text Color').size()) / len(training)\n# training['Profile Text Color'] = training['Profile Text Color'].apply(lambda x : enc_color_text[x])\n\n# enc_color_page = (training.groupby('Profile Page Color').size()) / len(training)\n# training['Profile Page Color'] = training['Profile Page Color'].apply(lambda x : enc_color_page[x])\n\n\ntraining[\"Profile Text Color\"].fillna(\"0084b4\",inplace = True)\nconv_p_t_c = []\nfor k in training[\"Profile Text Color\"]:\n    if(k != None and k != np.nan and k != 0):\n        conv_p_t_c.append(list(ImageColor.getcolor('#'+str(k), \"RGB\")))\n    else:\n        conv_p_t_c.append([219, 26, 44])\ntraining[['prof_t_c_r','prof_t_c_b','prof_t_c_g']]=conv_p_t_c\n\ntraining[\"Profile Page Color\"].fillna(\"ddeef6\",inplace = True)\nconv_p_p_c = []\nfor k in training[\"Profile Page Color\"]:\n    if(k != None and k != np.nan and k != 0):\n        conv_p_p_c.append(list(ImageColor.getcolor('#'+str(k), \"RGB\")))\n    else:\n        conv_p_p_c.append([221, 238, 246])\ntraining[['prof_p_p_r','prof_p_p_b','prof_p_p_g']]=conv_p_p_c\n\ntraining[\"Profile Theme Color\"].fillna(\"c0deed\",inplace = True)\nconv_p_tm_c = []\nfor k in training[\"Profile Theme Color\"]:\n    if(k != None and k != np.nan and k != 0):\n        conv_p_tm_c.append(list(ImageColor.getcolor('#'+str(k), \"RGB\")))\n    else:\n        conv_p_tm_c.append([192, 222, 237])\ntraining[['prof_p_tm_r','prof_p_tm_b','prof_p_tm_g']]=conv_p_tm_c\n\ntraining.drop(\"Profile Text Color\", 1,inplace=True)\ntraining.drop(\"Profile Page Color\", 1,inplace=True)\ntraining.drop(\"Profile Theme Color\", 1,inplace=True)\n\n\n\n\ntraining.set_index('Id',inplace=True)\n\ntraining['Profile Creation Timestamp'] = training['Profile Creation Timestamp'].apply(lambda x: (dateutil.parser.parse(x).replace(tzinfo=None)-datetime.datetime(1970,1,1)).days)\n\n\n\ntesting['Location Public Visibility'] = testing['Location Public Visibility'].str.lower()\n# User Name column is not relevent\ntesting.drop('User Name', 1,inplace=True)\ntesting.drop('User Time Zone', 1,inplace=True)\ntesting.drop('Profile Image', 1,inplace=True)\ntesting.drop('Location', 1,inplace=True)\n\n# mean fill nan for categorical features\ntesting['Profile Cover Image Status'].fillna('Set', inplace=True)\ntesting['UTC Offset'].fillna(testing['UTC Offset'].mean(), inplace=True)\ntesting['Profile Theme Color'].fillna(0,inplace = True)\ntesting['Profile Text Color'].fillna(0,inplace = True)\ntesting['Profile Page Color'].fillna(0,inplace = True)\n\n# One hot encodeing some of the categorical variables\ntesting = pd.get_dummies(testing, columns = ['Profile Verification Status'])\ntesting = pd.get_dummies(testing, columns = ['Location Public Visibility'])\n# testing = pd.get_dummies(testing, columns = ['Profile Category'])\n\n# Binary encoding some Cathegorical variables\ntesting['Personal URL'] = testing['Personal URL'].notnull().astype('int')\ntesting['Profile Cover Image Status'] = np.where(testing['Profile Cover Image Status']=='Set', 1, 0)\ntesting['Is Profile View Size Customized?'] = testing['Is Profile View Size Customized?'].astype(int)\n# testing['User Language'] = np.where(testing['User Language']=='en', 1, 0)\n\n# Median imputation for 'Avg Daily Profile Clicks'And'Avg Daily Profile Visit Duration in seconds'\nimp_median = SimpleImputer( strategy='median') #for median imputation replace 'mean' with 'median'\nimp_median.fit(testing[['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds']])\nimputed_train_df = imp_median.transform(testing[['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds']])\nimputed_train_df = pd.DataFrame(imputed_train_df,columns=['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds'])\ntesting[['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds']] = imputed_train_df[['Avg Daily Profile Clicks','Avg Daily Profile Visit Duration in seconds']]\n\n# target encoding utc, location, user language\ntesting['User Language'] = t_encod_prof_lang.transform(testing['User Language'])\ntesting['UTC Offset'] = t_encod_prof_utc.transform(testing['UTC Offset'])\ntesting['Profile Category'] = t_encod_prof_cat.transform(testing['Profile Category'])\n\n# Log transform of the numerical features\ntesting['Num of People Following'] = np.log(testing['Num of People Following'])\ntesting['Num of Status Updates'] = np.log(testing['Num of Status Updates'])\ntesting['Avg Daily Profile Clicks'] = np.log(testing['Avg Daily Profile Clicks'])\ntesting['Num of Followers'] = np.log(testing['Num of Followers'])\ntesting['Avg Daily Profile Visit Duration in seconds'] = np.log(testing['Avg Daily Profile Visit Duration in seconds'])\ntesting['Num of Direct Messages'] = np.log(testing['Num of Direct Messages'])\n\n# robust scale the nemerical features\ntesting = testing.replace([np.inf, -np.inf], 0)\n# scaler = preprocessing.RobustScaler()\n# robust_scaled_df = scaler.fit_transform(testing[['Num of People Following','Num of Status Updates','Avg Daily Profile Clicks','Num of Followers','Avg Daily Profile Visit Duration in seconds','Num of Direct Messages']])\n# testing[['Num of People Following','Num of Status Updates','Avg Daily Profile Clicks','Num of Followers','Avg Daily Profile Visit Duration in seconds','Num of Direct Messages']] = pd.DataFrame(robust_scaled_df, columns=['Num of People Following','Num of Status Updates','Avg Daily Profile Clicks','Num of Followers','Avg Daily Profile Visit Duration in seconds','Num of Direct Messages'])\n\n\n# UTF off-set frequency encoding\n# enc_time_zone = (testing.groupby('UTC Offset').size()) / len(testing)\n# testing['UTC Offset'] = testing['UTC Offset'].apply(lambda x : enc_time_zone[x])\n\n\n# colors encoding, convert to int\n#     testing['Profile Theme Color'].fillna(0,inplace = True)\n#     testing['Profile Theme Color']=testing['Profile Theme Color'].apply(lambda x: int(str(x), 16))\n\n#     testing['Profile Page Color'].fillna(0,inplace = True)\n#     testing['Profile Page Color']=testing['Profile Page Color'].apply(lambda x: int(str(x), 16))\n\n#     testing['Profile Text Color'].fillna(0,inplace = True)\n#     testing['Profile Text Color']=testing['Profile Text Color'].apply(lambda x: int(str(x), 16))\n\n# enc_color_theme = (testing.groupby('Profile Theme Color').size()) / len(testing)\n# testing['Profile Theme Color'] = testing['Profile Theme Color'].apply(lambda x : enc_color_theme[x])\n\n# enc_color_text = (testing.groupby('Profile Text Color').size()) / len(testing)\n# testing['Profile Text Color'] = testing['Profile Text Color'].apply(lambda x : enc_color_text[x])\n\n# enc_color_page = (testing.groupby('Profile Page Color').size()) / len(testing)\n# testing['Profile Page Color'] = testing['Profile Page Color'].apply(lambda x : enc_color_page[x])\n\ntesting[\"Profile Text Color\"].fillna(\"0084b4\",inplace = True)\nconv_p_t_ct = []\nfor k in testing[\"Profile Text Color\"]:\n    if(k != None and k != np.nan and k != 0):\n        conv_p_t_ct.append(list(ImageColor.getcolor('#'+str(k), \"RGB\")))\n    else:\n        conv_p_t_ct.append([219, 26, 44])\ntesting[['prof_t_c_r','prof_t_c_b','prof_t_c_g']]=conv_p_t_ct\n\ntesting[\"Profile Page Color\"].fillna(\"ddeef6\",inplace = True)\nconv_p_p_ct = []\nfor k in testing[\"Profile Page Color\"]:\n    if(k != None and k != np.nan and k != 0):\n        conv_p_p_ct.append(list(ImageColor.getcolor('#'+str(k), \"RGB\")))\n    else:\n        conv_p_p_ct.append([221, 238, 246])\ntesting[['prof_p_p_r','prof_p_p_b','prof_p_p_g']]=conv_p_p_ct\n\ntesting[\"Profile Theme Color\"].fillna(\"c0deed\",inplace = True)\nconv_p_tm_ct = []\nfor k in testing[\"Profile Theme Color\"]:\n    if(k != None and k != np.nan and k != 0):\n        conv_p_tm_ct.append(list(ImageColor.getcolor('#'+str(k), \"RGB\")))\n    else:\n        conv_p_tm_ct.append([192, 222, 237])\ntesting[['prof_p_tm_r','prof_p_tm_b','prof_p_tm_g']]=conv_p_tm_ct\n\ntesting.drop(\"Profile Text Color\", 1,inplace=True)\ntesting.drop(\"Profile Page Color\", 1,inplace=True)\ntesting.drop(\"Profile Theme Color\", 1,inplace=True)\n\n\ntesting.set_index('Id',inplace=True)\n\ntesting['Profile Creation Timestamp'] = testing['Profile Creation Timestamp'].apply(lambda x: (dateutil.parser.parse(x).replace(tzinfo=None)-datetime.datetime(1970,1,1)).days)\n\n\ntraining.astype('object').describe().transpose()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_col = ['Num of Profile Likes']\nx_col = training.columns.difference(['Num of Profile Likes'])\nx_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pearson's correlation feature selection \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n# define feature selection\nfs = SelectKBest(score_func=f_regression, k='all')\n# apply feature selection\nX_selected = fs.fit_transform(training[x_col], training[y_col].values.ravel())\nnames = training[x_col].columns.values[fs.get_support()]\nscores = fs.scores_[fs.get_support()]\nnames_scores = list(zip(names, scores))\nns_df = pd.DataFrame(data = names_scores, columns=['Feat_names', 'F_Scores'])\n#Sort the dataframe for better visualization\nns_df_sorted = ns_df.sort_values(['F_Scores', 'Feat_names'], ascending = [False, True])\nprint(ns_df_sorted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.astype('object').describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.drop(labels =['Profile Cover Image Status','Location Public Visibility_??'], axis =1,inplace=True)\ntesting.drop(labels =['Profile Cover Image Status','Location Public Visibility_??'], axis =1,inplace=True)\n# training.drop('Profile Category_', 1,inplace=True)\n# training.drop('Profile Cover Image Status', 1,inplace=True)\n\n# testing.drop('Location Public Visibility_??', 1,inplace=True)\n# # testing.drop('Profile Category_', 1,inplace=True)\n# testing.drop('Profile Cover Image Status', 1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( np.array(training[x_col]), np.array(training[y_col]), test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error as MSE \nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\n\n# https://www.datacamp.com/community/tutorials/xgboost-in-python\nxg_reg = make_pipeline(RobustScaler(),xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.01,\n                max_depth = 7, alpha = 10, n_estimators = 500,gamma=0.005, min_child_weight=1))\n\ncat_reg = make_pipeline(RobustScaler(),CatBoostRegressor(verbose=0, n_estimators=890,learning_rate=0.01,depth=7))\ngrad_reg = GradientBoostingRegressor(n_estimators=500,learning_rate=0.01)\n# kernel_reg = make_pipeline(StandardScaler(), KernelRidge())\nregr = make_pipeline(RobustScaler(), SVR(C=9, epsilon=0.2))\n\n\n# estimators = [\n#      ('lr', RidgeCV()),\n#      ('svr', LinearSVR(random_state=42)),\n#      ('svrnl', SVR(C=9, epsilon=0.2))\n# ]\n# stack_reg = make_pipeline(RobustScaler(), StackingRegressor(\n#     estimators=estimators,\n#     final_estimator=RandomForestRegressor(n_estimators=10, random_state=42)))\n# stack_reg.fit(X_train, y_train)\n\n\n\n# Random forest regressor  best: 2.364, e=2, d =10\n# best_e=0\n# best_d=0\n# best_score=1000\n# for e in [2,4,8,16,32,64,128,256,512,1024]:\n#     for d in [2,3,4,5,6,7,8,9,10]: \n#         print(e,d)\n#         rand_reg =  RandomForestRegressor(max_depth=d, n_estimators=e, random_state=0)\n#         rand_reg.fit(X_train, y_train)\n#         y_pred = rand_reg.predict(X_test)\n#         for i in range(len(y_test)):\n#         #     y_pred[i]=y_pred[i]\n#             if(y_pred[i]<0):\n#                 y_pred[i]=5390\n#         rmse = np.sqrt(mean_squared_log_error(y_test, y_pred))\n#         print(\"RMSE : % f\" %(rmse))\n#         if(rmse<best_score):\n#             best_score = rmse\n#             best_e = e\n#             best_d = d\n#         print(\"RMSE : % f\" %(best_score))\n#         print(best_e)\n#         print(best_d)\n#         print()\n         \n\n\n# for c in [1,2,3,4,5,6,7,8,9]:\n#     for s in [2,5,10,20,40,60,100]:\n#         print(c,s)\n#         lsvr = LinearSVR(random_state=s, C=c)\n#         lsvr.fit(X_train, y_train)\n\n#         y_pred = lsvr.predict(X_test)\n#         for i in range(len(y_test)):\n#             y_pred[i]=y_pred[i]\n#             if(y_pred[i]<0):\n#                 y_pred[i]=5390\n    \n\n# rmse = np.sqrt(mean_squared_log_error(y_test, y_pred)) \n# print(\"RMSE : % f\" %(rmse)) \n\n# for c in ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']:\n#     print(c)\n#     regr = make_pipeline(RobustScaler(), SVR(kernel=c,C=9, epsilon=0.1))\n#     regr.fit(X_train, y_train)\n#     y_pred = regr.predict(X_test)\n#     for i in range(len(y_test)):\n#         if(y_pred[i]<0):\n#             y_pred[i]=5390\n#         y_pred[i]=y_pred[i]\n\n#     rmse = np.sqrt(mean_squared_log_error(y_test, y_pred)) \n#     print(\"RMSE : % f\" %(rmse)) \n\n\n\n# minscore = 500\n# vec = np.zeros(2)\n# for m in range(500):\n#     randvec = np.random.rand(2)\n#     randvec = randvec / np.sum(randvec)\n#     er = VotingRegressor([('xg', xg_reg),('svr',regr)],weights=randvec)\n#     er.fit(X_train, y_train)\n\n#     y_pred = er.predict(X_test)\n\n#     for i in range(len(y_test)):\n#         if(y_pred[i]<0):\n#             y_pred[i]=5390\n#         y_pred[i]=y_pred[i]\n\n#     rmse = np.sqrt(mean_squared_log_error(y_test, y_pred)) \n#     print(\"RMSE : % f\" %(rmse)) \n#     print(randvec)\n#     if(minscore > rmse):\n#         minscore = rmse\n#         vec = randvec\n#     print(minscore)\n#     print(vec)\n\n\n# training model:\n\nrand_reg =  RandomForestRegressor(max_depth=10, n_estimators=2, random_state=0)\n\nrand_reg.fit(X_train, y_train)\ny_pred = rand_reg.predict(X_test)\nfor i in range(len(y_test)):\n#     y_pred[i]=y_pred[i]\n    if(y_pred[i]<0):\n        y_pred[i]=5390\nrmse = np.sqrt(mean_squared_log_error(y_test, y_pred)) \nprint(\"RMSE : % f\" %(rmse)) \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[0.44153715 0.67033311 0.7148971 ]\n2.6197931110765706"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\nfor i in range(len(y_test)):\n    if(y_test[i]<0):\n        y_test[i]=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.01,\n#                 'max_depth': 7, 'alpha': 10}\n\n# cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n#                     num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(er, X_train, y_train, cv=kfold)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing.astype('object').describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rand_reg.predict(np.array(testing[x_col]))\nfor i in range(len(y_pred)):\n    if(y_pred[i]<0):\n        y_pred[i]=5390\ndummy = pd.DataFrame()\ndummy[\"Id\"] = testing.index\ndummy[\"Predicted\"]=y_pred\ndummy.to_csv(\"submission.csv\", sep=',' ,index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}